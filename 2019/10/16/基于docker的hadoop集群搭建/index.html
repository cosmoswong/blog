<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.9.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="基于docker的hadoop集群搭建"><meta name="keywords" content="dock,hadoop"><meta name="author" content="cosmoswong"><meta name="copyright" content="cosmoswong"><title>基于docker的hadoop集群搭建 | cosmoswong</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.6.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.6.1"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css?version=1.6.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"QLGXU0B37P","apiKey":"07b1b39fd26c4dda2d2fdba30e4126fb","indexName":"blog","hits":{"per_page":10},"languages":{"input_placeholder":"搜索文章","hits_empty":"找不到您查询的内容:${query}","hits_stats":"找到 ${hits} 条结果，用时 ${time} 毫秒"}},
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  }
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Dcoker中完全分布式集群的搭建"><span class="toc-text">Dcoker中完全分布式集群的搭建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1、基础工作"><span class="toc-text">1、基础工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1）、-下载centos"><span class="toc-text">1.1）、 下载centos</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2）、安装和配置ssh"><span class="toc-text">1.2）、安装和配置ssh:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3）、配置SSH免密登录"><span class="toc-text">1.3）、配置SSH免密登录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2、安装和部署Hadoop集群"><span class="toc-text">2、安装和部署Hadoop集群</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1）、运行镜像"><span class="toc-text">2.1）、运行镜像</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2）、修改配置文件"><span class="toc-text">2.2）、修改配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2）、启动集群"><span class="toc-text">2.2）、启动集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3）、验证结果"><span class="toc-text">2.3）、验证结果</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">cosmoswong</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/cosmoswong">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">12</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">10</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/background.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">cosmoswong</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">主页</a><a class="site-page" href="/archives">时间轴</a><a class="site-page" href="/tags">标签</a><a class="site-page" href="/categories">分类</a><a class="site-page" href="/about">关于</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">基于docker的hadoop集群搭建</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2019-10-16</time><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.1k</span><span class="post-meta__separator">|</span><span>阅读时长: 11 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>序：在以往操作中，需要在每台设备上搭建Hadoop、Hive和hbase环境，不仅操作麻烦而且繁琐。而在docker中一台设备上就可以完成，而且操作方便。</p>
<h1 id="Dcoker中完全分布式集群的搭建"><a href="#Dcoker中完全分布式集群的搭建" class="headerlink" title="Dcoker中完全分布式集群的搭建"></a>Dcoker中完全分布式集群的搭建</h1><p>   这里所有操作都是在虚拟机192.168.137.25上完成，已经事先完成安装docker。如果没有安装docker请先<a href="https://blog.csdn.net/u013140345/article/details/79771208" target="_blank" rel="noopener">安装docker</a>.</p>
<h2 id="1、基础工作"><a href="#1、基础工作" class="headerlink" title="1、基础工作"></a>1、基础工作</h2><h3 id="1-1）、-下载centos"><a href="#1-1）、-下载centos" class="headerlink" title="1.1）、 下载centos"></a>1.1）、 下载centos</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@Linux4 ~]# docker pull  centos     </span><br><span class="line">Using default tag: latest</span><br><span class="line">Trying to pull repository docker.io/library/centos ... </span><br><span class="line">latest: Pulling from docker.io/library/centos</span><br></pre></td></tr></table></figure>

<p><img src="https://cosmoswong-img-1258890093.cos.ap-shanghai.myqcloud.com/images/1571234684095.png" alt="1571234684095"></p>
<p>只是这个centos实在是太简陋了，需要安装的软件有：ssh、which</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum -y install passwd openssl openssh-server </span><br><span class="line">yum -y install which</span><br></pre></td></tr></table></figure>

<h3 id="1-2）、安装和配置ssh"><a href="#1-2）、安装和配置ssh" class="headerlink" title="1.2）、安装和配置ssh:"></a>1.2）、安装和配置ssh:</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]# yum -y install passwd openssl openssh-server  openssh-clients</span><br><span class="line">[root@master module]# mkdir  /var/run/sshd/</span><br><span class="line">[root@master module]# vi /etc/ssh/sshd_config </span><br><span class="line">注释改行</span><br><span class="line"><span class="meta">#</span><span class="bash">UsePAM yes</span></span><br><span class="line">也可以使用下面的方式来操作</span><br><span class="line">[root@master module]# sed -i "s/UsePAM.*/UsePAM no/g" /etc/ssh/sshd_config</span><br><span class="line">UsePAM no</span><br><span class="line">[root@master module]# /usr/sbin/sshd -D &amp;</span><br><span class="line"> [root@master module]# ps -ef | grep sshd</span><br><span class="line"> root       259     1  0 02:44 ?        00:00:00 /usr/sbin/sshd -D</span><br><span class="line"> root       261     1  0 02:45 ?        00:00:00 grep --color=auto sshd</span><br></pre></td></tr></table></figure>

<p><a href="https://blog.csdn.net/u013140345/article/details/79777311" target="_blank" rel="noopener">https://blog.csdn.net/u013140345/article/details/79777311</a></p>
<p>（centos7）创建支持ssh服务的docker镜像</p>
<h3 id="1-3）、配置SSH免密登录"><a href="#1-3）、配置SSH免密登录" class="headerlink" title="1.3）、配置SSH免密登录"></a>1.3）、配置SSH免密登录</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@master module]# ssh-keygen -t rsa</span><br><span class="line">[root@master module]# cd ~/.ssh</span><br><span class="line">[root@master module]# cat id_rsa.pub &gt;&gt; authorized_keys</span><br></pre></td></tr></table></figure>

<h2 id="2、安装和部署Hadoop集群"><a href="#2、安装和部署Hadoop集群" class="headerlink" title="2、安装和部署Hadoop集群"></a>2、安装和部署Hadoop集群</h2><p>集群的部署信息：</p>
<table>
<thead>
<tr>
<th align="center">设备列表</th>
<th align="center">HDFS</th>
<th align="center">排行1</th>
</tr>
</thead>
<tbody><tr>
<td align="center">172.0.0.2</td>
<td align="center">NameNode/DataNode</td>
<td align="center">ResouceManager/NodeManager</td>
</tr>
<tr>
<td align="center">172.0.0.3</td>
<td align="center">DataNode</td>
<td align="center">NodeManager</td>
</tr>
<tr>
<td align="center">172.0.0.4</td>
<td align="center">SecondNameNode/DataNode</td>
<td align="center">NodeManager</td>
</tr>
</tbody></table>
<h3 id="2-1）、运行镜像"><a href="#2-1）、运行镜像" class="headerlink" title="2.1）、运行镜像"></a>2.1）、运行镜像</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@Linux5 ~]# docker run -it --rm  -v /opt/software:/opt/software -h master  --name master centos-hadoop  /bin/bash</span><br></pre></td></tr></table></figure>

<p>关于参数的解释：</p>
<ul>
<li><p>-v：表示创建数据卷，用于容器和宿主机之间的数据共享。 冒号前是/opt/software是宿主机目录，冒号后是容器内的目录。</p>
</li>
<li><p>-h：设置主机名为master</p>
</li>
</ul>
<p>启动后进入到容器</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@master /]# cd /opt</span><br><span class="line">[root@master opt]# mkdir module</span><br><span class="line">[root@master opt]# ls -l</span><br><span class="line">total 0</span><br><span class="line">drwxr-xr-x. 2 root root   6 Oct 18 02:27 module</span><br><span class="line">drwxrwxrwx. 2 root root 195 Oct 16 11:19 software</span><br><span class="line">[root@master opt]# cd software/</span><br><span class="line">[root@master software]# ls -l</span><br><span class="line">total 1755528</span><br><span class="line">-rwxrwxrwx. 1 root root 197657687 May 22  2017 hadoop-2.7.2.tar.gz</span><br><span class="line">-rwxrwxrwx. 1 root root 185515842 Aug 19  2017 jdk-8u144-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 解压安装hadoop和JAVA</span></span><br><span class="line">[root@master software]# tar -zxvf hadoop-2.7.2.tar.gz -C /opt/module  </span><br><span class="line">[root@master software]# tar -zxvf jdk-8u144-linux-x64.tar.gz -C /opt/module </span><br><span class="line">[root@master software]# cd /opt/module/</span><br><span class="line">[root@master module]# ls</span><br><span class="line">hadoop-2.7.2  jdk1.8.0_144</span><br><span class="line">[root@master module]# mv hadoop-2.7.2 hadoop       </span><br><span class="line">[root@master module]# mv jdk1.8.0_144 jdk1.8 </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 配置环境变量</span></span><br><span class="line">[root@master module]# vi /etc/profile</span><br><span class="line"><span class="meta">#</span><span class="bash">JAVA_HOME</span></span><br><span class="line">export JAVA_HOME=/opt/module/jdk1.8.</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">#HADOOP_HOME</span></span></span><br><span class="line">export HADOOP_HOME=/opt/module/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看配置信息</span></span><br><span class="line">[root@master module]# source /etc/profile</span><br><span class="line">[root@master module]# echo $JAVA_HOME</span><br><span class="line">/opt/module/jdk1.8</span><br><span class="line">[root@master module]# echo $HADOOP_HOME</span><br><span class="line">/opt/module/hadoop</span><br><span class="line">[root@master module]# java -version</span><br><span class="line">java version "1.8.0_144"</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_144-b01)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.144-b01, mixed mode)</span><br></pre></td></tr></table></figure>

<h3 id="2-2）、修改配置文件"><a href="#2-2）、修改配置文件" class="headerlink" title="2.2）、修改配置文件"></a>2.2）、修改配置文件</h3><ol>
<li>core-site.xml文件：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://master:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/opt/module/hadoop/data/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>hadoop-env.sh：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>hdfs-site.xml：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;slave2:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>yarn-env.sh：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>yarn-site.xml：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;slave1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="6">
<li>mapred-env.sh：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/opt/module/jdk1.8</span><br></pre></td></tr></table></figure>

<ol start="7">
<li>mapred-site.xml：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">       &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>

<ol start="8">
<li>slaves：</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure>

<p>现在存在两个问题必须要解决</p>
<ul>
<li>Docker容器中的IP地址是启动后自动分配的，且不能手动更改。好在每次重启容器后，IP地址基本不发生变化，这个暂时不需要考虑。</li>
</ul>
<p>*　hostname、host配置在容器内修改了，只能在本次容器声明周期内有效，如果容器退出了，重新启动，这两个配置会被还原，且这两个配置无法通过commit命令写入镜像。</p>
<p>　　为了解决第二个问题，我们可以通过下面的脚本，来实现每次进入到容器时先执行这个脚本，添加映射信息到/etc/hosts文件。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line">echo 172.17.0.2 master &gt;&gt; /etc/hosts</span><br><span class="line">echo 172.17.0.3 slave1 &gt;&gt; /etc/hosts</span><br><span class="line">echo 172.17.0.4 slave2 &gt;&gt; /etc/hosts</span><br><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p>注意：上面的所有操作都是在容器中完成的，如果容器重启，则配置丢失，为了解决这个问题，可以先进行commit操作。</p>
<p>在宿主机上执行：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@Linux5 opt]# docker ps </span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                                                                        NAMES</span><br><span class="line"><span class="meta">#</span><span class="bash"> 就是这ID</span></span><br><span class="line">156973c4491d        228dfd812b66        "/bin/bash"         3 hours ago         Up 3 hours          8088/tcp, 19888/tcp, 50070/tcp                                               slave2</span><br><span class="line">38832bc51040        228dfd812b66        "/bin/bash"         3 hours ago         Up 3 hours          8088/tcp, 19888/tcp, 50070/tcp                                               slave1</span><br><span class="line">9bd4e9511f71        228dfd812b66        "/bin/bash"         3 hours ago         Up 3 hours          0.0.0.0:8088-&gt;8088/tcp, 0.0.0.0:19888-&gt;19888/tcp, 0.0.0.0:50070-&gt;50070/tcp   master</span><br><span class="line">[root@Linux5 opt]# docker commit 156973c4491d centos-hadoop:1.3</span><br></pre></td></tr></table></figure>

<h3 id="2-2）、启动集群"><a href="#2-2）、启动集群" class="headerlink" title="2.2）、启动集群"></a>2.2）、启动集群</h3><p>在master上启动hdfs</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# sbin/start-dfs.sh</span><br><span class="line">Starting namenodes on [master]</span><br><span class="line">master: starting namenode, logging to /opt/module/hadoop/logs/hadoop-root-namenode-master.out</span><br><span class="line">slave2: starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-slave2.out</span><br><span class="line">slave1: starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-slave1.out</span><br><span class="line">master: starting datanode, logging to /opt/module/hadoop/logs/hadoop-root-datanode-master.out</span><br><span class="line">Starting secondary namenodes [slave2]</span><br><span class="line">slave2: starting secondarynamenode, logging to /opt/module/hadoop/logs/hadoop-root-secondarynamenode-slave2.out</span><br></pre></td></tr></table></figure>

<p>在slave1启动Yarn</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 hadoop]# sbin/start-yarn.sh</span><br><span class="line">starting yarn daemons</span><br><span class="line">starting resourcemanager, logging to /opt/module/hadoop/logs/yarn-root-resourcemanager-slave1.out</span><br><span class="line">slave2: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-slave2.out</span><br><span class="line">master: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-master.out</span><br><span class="line">slave1: starting nodemanager, logging to /opt/module/hadoop/logs/yarn-root-nodemanager-slave1.out</span><br></pre></td></tr></table></figure>

<p>查看master节点的进程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# jps</span><br><span class="line">3585 NameNode</span><br><span class="line">3993 NodeManager</span><br><span class="line">3691 DataNode</span><br></pre></td></tr></table></figure>

<p>查看slave1节点的进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 hadoop]# jps</span><br><span class="line">1748 DataNode</span><br><span class="line">2004 ResourceManager</span><br><span class="line">2107 NodeManager</span><br></pre></td></tr></table></figure>

<p>查看slave2节点的进程</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@slave2 logs]# jps</span><br><span class="line">1555 DataNode</span><br><span class="line">1655 SecondaryNameNode</span><br><span class="line">1790 NodeManager</span><br><span class="line">[root@slave2 logs]#</span><br></pre></td></tr></table></figure>

<h3 id="2-3）、验证结果"><a href="#2-3）、验证结果" class="headerlink" title="2.3）、验证结果"></a>2.3）、验证结果</h3><p>访问界面：<a href="http://192.168.137.25:50070/explorer.html#/&gt;" target="_blank" rel="noopener">http://192.168.137.25:50070/explorer.html#/&gt;</a></p>
<p><img src="https://cosmoswong-img-1258890093.cos.ap-shanghai.myqcloud.com/images/1571388178409.png" alt="1571388178409"></p>
<p>如果通过上面的方式不能访问，请安按照如下的顺序检查：</p>
<p>(1)  curl <a href="http://172.17.0.2:50070/" target="_blank" rel="noopener">http://172.17.0.2:50070/</a>    在master节点上访问，查看是否能够打印报文信息</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]#  curl http://172.17.0.2:50070/  </span><br><span class="line">&lt;!--</span><br><span class="line">   Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line">   contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">   this work for additional information regarding copyright ownership.</span><br><span class="line">   The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line">   (the "License"); you may not use this file except in compliance with</span><br><span class="line">   the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">       http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">   Unless required by applicable law or agreed to in writing, software</span><br><span class="line">   distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line">   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">   See the License for the specific language governing permissions and</span><br><span class="line">   limitations under the License.</span><br><span class="line"><span class="meta">--&gt;</span><span class="bash"></span></span><br><span class="line">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"</span><br><span class="line">    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&gt;</span><br><span class="line">&lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;</span><br><span class="line">&lt;head&gt;</span><br><span class="line">&lt;meta http-equiv="REFRESH" content="0;url=dfshealth.html" /&gt;</span><br><span class="line">&lt;title&gt;Hadoop Administration&lt;/title&gt;</span><br><span class="line">&lt;/head&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p>如果不能访问，检查进程是否正常。</p>
<p>（2）curl <a href="http://192.168.137.25:50070/" target="_blank" rel="noopener">http://192.168.137.25:50070/</a> 能够打印报文头信息，说明OK</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@Linux5 opt]# curl http://192.168.137.25:50070/</span><br><span class="line"> &lt;!--</span><br><span class="line">    Licensed to the Apache Software Foundation (ASF) under one or more</span><br><span class="line">    contributor license agreements.  See the NOTICE file distributed with</span><br><span class="line">    this work for additional information regarding copyright ownership.</span><br><span class="line">    The ASF licenses this file to You under the Apache License, Version 2.0</span><br><span class="line">    (the "License"); you may not use this file except in compliance with</span><br><span class="line">    the License.  You may obtain a copy of the License at</span><br><span class="line"> </span><br><span class="line">        http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> </span><br><span class="line">    Unless required by applicable law or agreed to in writing, software</span><br><span class="line">    distributed under the License is distributed on an "AS IS" BASIS,</span><br><span class="line">    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">    See the License for the specific language governing permissions and</span><br><span class="line">    limitations under the License.</span><br><span class="line"><span class="meta"> --&gt;</span><span class="bash"></span></span><br><span class="line"> &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"</span><br><span class="line">     "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"&gt;</span><br><span class="line"> &lt;html xmlns="http://www.w3.org/1999/xhtml"&gt;</span><br><span class="line"> &lt;head&gt;</span><br><span class="line"> &lt;meta http-equiv="REFRESH" content="0;url=dfshealth.html" /&gt;</span><br><span class="line"> &lt;title&gt;Hadoop Administration&lt;/title&gt;</span><br><span class="line"> &lt;/head&gt;</span><br><span class="line"> &lt;/html&gt;</span><br></pre></td></tr></table></figure>

<p>（3）如果上面两步都正常，在win10上通过浏览器还是不能，可能是由于没有开启IP转发所导致的</p>
<p>解决方法就是开启IP转发：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@Linux5 opt]# cat /proc/sys/net/ipv4/ip_forward</span><br><span class="line">0</span><br><span class="line">[root@Linux5 opt]# sysctl -w net.ipv4.ip_forward=1</span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure>

<p>但是这样只是临时生效，如果想要永久生效，则需要这样做：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@Linux5 opt]# vi /etc/sysctl.conf </span><br><span class="line">net.ipv4.ip_forward = 1</span><br></pre></td></tr></table></figure>

<blockquote>
<p>参考链接：<a href="https://blog.csdn.net/wo18237095579/article/details/81509364" target="_blank" rel="noopener">问题 - Linux 下 Docker 端口映射到宿主机后 外部无法访问对应宿主机端口</a></p>
</blockquote>
<p>引用连接：</p>
<blockquote>
<p>（centos7）搭建基于docker的hadoop集群1：<a href="https://blog.csdn.net/u013140345/article/details/79771208" target="_blank" rel="noopener">https://blog.csdn.net/u013140345/article/details/79771208</a></p>
</blockquote>
<blockquote>
<p>（centos7）搭建基于docker的hadoop集群2：<a href="https://blog.csdn.net/u013140345/article/details/79773212" target="_blank" rel="noopener">https://blog.csdn.net/u013140345/article/details/79773212</a></p>
</blockquote>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">cosmoswong</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://cosmoswong.github.io/2019/10/16/基于docker的hadoop集群搭建/">https://cosmoswong.github.io/2019/10/16/基于docker的hadoop集群搭建/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://cosmoswong.github.io">cosmoswong</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/dock/">dock</a><a class="post-meta__tags" href="/tags/hadoop/">hadoop</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="https://cosmoswong-img-1258890093.cos.ap-shanghai.myqcloud.com/images/1789122396123109.png"><div class="post-qr-code__desc">微信打赏</div></div></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2019/10/16/提高工作效率的桌面工具推荐/"><i class="fa fa-chevron-left">  </i><span>提高工作效率的桌面工具推荐</span></a></div><div class="next-post pull-right"><a href="/2019/10/14/hadoop学习简记/"><span>hadoop学习简记</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: '87407cc4be7e22caa176',
  clientSecret: '85104a46339a6b7ff75b040fc11aba2211fc1450',
  repo: 'CommentRepository',
  owner: 'cosmoswong',
  admin: 'cosmoswong',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/background.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2015 - 2019 By cosmoswong</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><span class="fa">阅读量:<span><span id="busuanzi_value_page_pv"></span><span></span></span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.6.1"></script><script src="/js/fancybox.js?version=1.6.1"></script><script src="/js/sidebar.js?version=1.6.1"></script><script src="/js/copy.js?version=1.6.1"></script><script src="/js/fireworks.js?version=1.6.1"></script><script src="/js/transition.js?version=1.6.1"></script><script src="/js/scroll.js?version=1.6.1"></script><script src="/js/head.js?version=1.6.1"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"superSample":2,"width":210,"height":420,"position":"right","hOffset":0,"vOffset":-20},"log":false,"tagMode":false});</script></body></html>